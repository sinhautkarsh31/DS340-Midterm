{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "## Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load datasets\n",
    "movies = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\Github\\DS340-Midterm\\Small MovieLens\\movies.csv')\n",
    "ratings = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\Github\\DS340-Midterm\\Small MovieLens\\ratings.csv')\n",
    "tags = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\Github\\DS340-Midterm\\Small MovieLens\\tags.csv')\n",
    "\n",
    "# Keep necessary columns\n",
    "movies = movies[['movieId', 'title', 'genres']]\n",
    "ratings = ratings[['userId', 'movieId', 'rating']]\n",
    "tags = tags[['movieId', 'tag']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Movie Data\n",
    "\n",
    "Extract year, clean titles, and combine genres and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Function to extract year from title\n",
    "def extract_year(title):\n",
    "    match = re.search(r'\\((\\d{4})\\)', title)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to create 'year' column\n",
    "movies['year'] = movies['title'].apply(extract_year)\n",
    "\n",
    "# Clean the 'title' by removing the year and converting to lowercase\n",
    "movies['title_clean'] = movies['title'].apply(lambda x: re.sub(r'\\s*\\(\\d{4}\\)', '', x).lower())\n",
    "\n",
    "# Group tags by 'movieId' and concatenate them into a single string\n",
    "tags_grouped = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Merge tags with movies\n",
    "movies = pd.merge(movies, tags_grouped, on='movieId', how='left')\n",
    "movies['tag'] = movies['tag'].fillna('')\n",
    "\n",
    "# Combine genres, title_clean, tags, and year into the 'related' column\n",
    "movies['year_str'] = movies['year'].astype(str)\n",
    "movies['related'] = movies['genres'].str.replace('|', ' ') + ' ' + movies['title_clean'] + ' ' + movies['tag'] + ' ' + movies['year_str']\n",
    "\n",
    "# Preprocess the 'related' column\n",
    "movies['related'] = movies['related'].str.lower()\n",
    "movies['related'] = movies['related'].str.replace(r'\\d+', '', regex=True)\n",
    "movies['related'] = movies['related'].str.replace(r'[^a-z\\s]', '', regex=True)\n",
    "movies['related'] = movies['related'].str.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data into Training, Testing, and Validation Sets\n",
    "We'll create training, testing, and validation sets for both CF and CBF models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Ratings Data (CF Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split 10% of the data for validation\n",
    "cf_remaining, cf_validation = train_test_split(\n",
    "    ratings, test_size=0.1, random_state=42, stratify=ratings['userId']\n",
    ")\n",
    "\n",
    "# Split remaining data into training and testing sets (80% train, 20% test)\n",
    "cf_train, cf_test = train_test_split(\n",
    "    cf_remaining, test_size=0.2, random_state=42, stratify=cf_remaining['userId']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Movies Data (CBF Model)\n",
    "\n",
    "Since CBF relies on item features, we'll split the movies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split 10% of the movies for validation\n",
    "cbf_remaining, cbf_validation = train_test_split(\n",
    "    movies, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Split remaining data into training and testing sets (80% train, 20% test)\n",
    "cbf_train, cbf_test = train_test_split(\n",
    "    cbf_remaining, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Content-Based Filtering (CBF Model)\n",
    "## Step 1: Vectorize Item Content\n",
    "Use TF-IDF to vectorize the 'related' column in cbf_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer with English stop words\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the 'related' column\n",
    "tfidf_matrix = tfidf.fit_transform(cbf_train['related'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build User Profiles\n",
    "Create user profiles by aggregating the content of the movies they've rated.\n",
    "\n",
    "### a. Merge Ratings with Movie Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge cf_train with cbf_train to get 'related' content\n",
    "user_ratings = pd.merge(cf_train, cbf_train[['movieId', 'related']], on='movieId', how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Transform 'related' Conten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the 'related' column in user_ratings\n",
    "user_ratings_tfidf = tfidf.transform(user_ratings['related'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Build User Profiles\n",
    "Aggregate the TF-IDF vectors weighted by the user's ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create a dictionary to store user profiles\n",
    "user_profiles = {}\n",
    "\n",
    "# Get unique user IDs\n",
    "user_ids = user_ratings['userId'].unique()\n",
    "\n",
    "for user_id in user_ids:\n",
    "    # Get indices of movies rated by the user\n",
    "    indices = user_ratings[user_ratings['userId'] == user_id].index\n",
    "    \n",
    "    # Get the TF-IDF vectors and ratings\n",
    "    tfidf_vectors = user_ratings_tfidf[indices]\n",
    "    ratings_values = user_ratings.loc[indices, 'rating'].values.reshape(-1, 1)\n",
    "    \n",
    "    # Compute the weighted average\n",
    "    weighted_tfidf = tfidf_vectors.multiply(ratings_values)\n",
    "    user_profile = weighted_tfidf.mean(axis=0)\n",
    "    \n",
    "    user_profiles[user_id] = user_profile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Predict Ratings\n",
    "Predict ratings for user-item pairs in cf_test.\n",
    "\n",
    "### a. Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge cf_test with cbf_test to get 'related' content\n",
    "test_data = pd.merge(cf_test, cbf_test[['movieId', 'related']], on='movieId', how='inner')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Transform 'related' Content in Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the 'related' column in test_data\n",
    "test_tfidf = tfidf.transform(test_data['related'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Predict Ratings\n",
    "Compute the dot product between user profiles and item vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_ratings = []\n",
    "\n",
    "for idx, row in test_data.iterrows():\n",
    "    user_id = row['userId']\n",
    "    movie_tfidf = test_tfidf[idx]  # This is a sparse matrix\n",
    "    user_profile = user_profiles.get(user_id)  # This is also a sparse matrix\n",
    "    \n",
    "    if user_profile is not None:\n",
    "        # Convert both user_profile and movie_tfidf to dense arrays\n",
    "        user_profile_dense = user_profile.A.flatten()  # Convert to 1D array\n",
    "        movie_tfidf_dense = movie_tfidf.toarray().flatten()  # Convert to 1D array\n",
    "        \n",
    "        # Compute dot product and append as float\n",
    "        pred_rating = np.dot(user_profile_dense, movie_tfidf_dense)  # Dot product\n",
    "        predicted_ratings.append(float(pred_rating))\n",
    "    else:\n",
    "        # Assign NaN if user profile is not available\n",
    "        predicted_ratings.append(np.nan)\n",
    "\n",
    "test_data['predicted_rating'] = predicted_ratings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Scale Predicted Ratings\n",
    "Scale the predicted ratings to match the actual rating scale (0.5 to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Remove NaN values\n",
    "valid_predictions = test_data.dropna(subset=['predicted_rating'])\n",
    "\n",
    "# Scale predicted ratings\n",
    "scaler = MinMaxScaler(feature_range=(0.5, 5))\n",
    "scaled_ratings = scaler.fit_transform(valid_predictions['predicted_rating'].values.reshape(-1, 1))\n",
    "valid_predictions['predicted_rating'] = scaled_ratings.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the CBF Model\n",
    "Compute RMSE and MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate the CBF Model\n",
    "Compute RMSE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBF Model RMSE: 2.5547\n",
      "CBF Model MAE: 2.3343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pedro\\Desktop\\Github\\DS_340-Movies\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "rmse = mean_squared_error(valid_predictions['rating'], valid_predictions['predicted_rating'], squared=False)\n",
    "mae = mean_absolute_error(valid_predictions['rating'], valid_predictions['predicted_rating'])\n",
    "\n",
    "print(f\"CBF Model RMSE: {rmse:.4f}\")\n",
    "print(f\"CBF Model MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Precision, Recall, and F1-score\n",
    "Define relevant items as those with ratings ≥ 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBF Model Precision: 1.0000\n",
      "CBF Model Recall: 0.0026\n",
      "CBF Model F1-Score: 0.0052\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define relevance\n",
    "valid_predictions['actual_relevant'] = valid_predictions['rating'] >= 4.0\n",
    "valid_predictions['predicted_relevant'] = valid_predictions['predicted_rating'] >= 4.0\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(valid_predictions['actual_relevant'], valid_predictions['predicted_relevant'])\n",
    "recall = recall_score(valid_predictions['actual_relevant'], valid_predictions['predicted_relevant'])\n",
    "f1 = f1_score(valid_predictions['actual_relevant'], valid_predictions['predicted_relevant'])\n",
    "\n",
    "print(f\"CBF Model Precision: {precision:.4f}\")\n",
    "print(f\"CBF Model Recall: {recall:.4f}\")\n",
    "print(f\"CBF Model F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Collaborative Filtering (CF) Model\n",
    "We'll use the Surprise library to implement a CF model using SVD.\n",
    "\n",
    "## Step 1: Prepare Data\n",
    "Convert cf_train and cf_test into Surprise data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split as surprise_train_test_split\n",
    "\n",
    "# Prepare data for Surprise\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Create a validation set\n",
    "train_data, validation_data = train_test_split(ratings, test_size=0.1, random_state=42, stratify=ratings['userId'])\n",
    "\n",
    "# Load training data into Surprise format\n",
    "trainset_full = Dataset.load_from_df(train_data[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "# Perform Surprise train-test split on training data\n",
    "trainset = trainset_full.build_full_trainset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Train CF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x1fef0ab9750>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize SVD algorithm\n",
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset\n",
    "algo.fit(trainset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Predict Ratings\n",
    "Predict ratings for user-item pairs in cf_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare testset for prediction\n",
    "testset = list(zip(cf_test['userId'], cf_test['movieId'], cf_test['rating']))\n",
    "\n",
    "# Get predictions\n",
    "predictions = algo.test(testset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate the CF Model\n",
    "Extract Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert predictions to DataFrame\n",
    "pred_df = pd.DataFrame([(pred.uid, pred.iid, pred.r_ui, pred.est) for pred in predictions],\n",
    "                        columns=['userId', 'movieId', 'rating', 'predicted_rating'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute RMSE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF Model RMSE: 0.6269\n",
      "CF Model MAE: 0.4892\n"
     ]
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "rmse = accuracy.rmse(predictions, verbose=False)\n",
    "mae = accuracy.mae(predictions, verbose=False)\n",
    "\n",
    "print(f\"CF Model RMSE: {rmse:.4f}\")\n",
    "print(f\"CF Model MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Precision, Recall, and F1-score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF Model Precision: 0.9308\n",
      "CF Model Recall: 0.4589\n",
      "CF Model F1-Score: 0.6147\n"
     ]
    }
   ],
   "source": [
    "# Define relevance\n",
    "pred_df['actual_relevant'] = pred_df['rating'] >= 4.0\n",
    "pred_df['predicted_relevant'] = pred_df['predicted_rating'] >= 4.0\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(pred_df['actual_relevant'], pred_df['predicted_relevant'])\n",
    "recall = recall_score(pred_df['actual_relevant'], pred_df['predicted_relevant'])\n",
    "f1 = f1_score(pred_df['actual_relevant'], pred_df['predicted_relevant'])\n",
    "\n",
    "print(f\"CF Model Precision: {precision:.4f}\")\n",
    "print(f\"CF Model Recall: {recall:.4f}\")\n",
    "print(f\"CF Model F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hybrid Model\n",
    "Combine predictions from CBF and CF models.\n",
    "\n",
    "## Step 1: Merge Predictions\n",
    "Merge valid_predictions from CBF and pred_df from CF on userId and movieId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge predictions\n",
    "hybrid_df = pd.merge(valid_predictions[['userId', 'movieId', 'predicted_rating']], \n",
    "                        pred_df[['userId', 'movieId', 'predicted_rating']], \n",
    "                        on=['userId', 'movieId'], \n",
    "                        suffixes=('_cbf', '_cf'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Combine Predictions\n",
    "Use a weighted average to combine the predicted ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign weights\n",
    "weight_cbf = 0.5\n",
    "weight_cf = 0.5\n",
    "\n",
    "# Compute hybrid predicted rating\n",
    "hybrid_df['predicted_rating'] = (weight_cbf * hybrid_df['predicted_rating_cbf'] + \n",
    "                                 weight_cf * hybrid_df['predicted_rating_cf'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Evaluate the Hybrid Model\n",
    "Merge with Actual Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge hybrid predictions with actual ratings\n",
    "hybrid_df = pd.merge(hybrid_df, cf_test[['userId', 'movieId', 'rating']], on=['userId', 'movieId'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute RMSE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model RMSE: 1.4022\n",
      "Hybrid Model MAE: 1.2378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pedro\\Desktop\\Github\\DS_340-Movies\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(hybrid_df['rating'], hybrid_df['predicted_rating'], squared=False)\n",
    "mae = mean_absolute_error(hybrid_df['rating'], hybrid_df['predicted_rating'])\n",
    "\n",
    "print(f\"Hybrid Model RMSE: {rmse:.4f}\")\n",
    "print(f\"Hybrid Model MAE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Precision, Recall, and F1-score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model Precision: 1.0000\n",
      "Hybrid Model Recall: 0.0032\n",
      "Hybrid Model F1-Score: 0.0065\n"
     ]
    }
   ],
   "source": [
    "# Define relevance\n",
    "hybrid_df['actual_relevant'] = hybrid_df['rating'] >= 4.0\n",
    "hybrid_df['predicted_relevant'] = hybrid_df['predicted_rating'] >= 4.0\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(hybrid_df['actual_relevant'], hybrid_df['predicted_relevant'])\n",
    "recall = recall_score(hybrid_df['actual_relevant'], hybrid_df['predicted_relevant'])\n",
    "f1 = f1_score(hybrid_df['actual_relevant'], hybrid_df['predicted_relevant'])\n",
    "\n",
    "print(f\"Hybrid Model Precision: {precision:.4f}\")\n",
    "print(f\"Hybrid Model Recall: {recall:.4f}\")\n",
    "print(f\"Hybrid Model F1-Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Final validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CF Model Validation RMSE: 0.8619\n",
      "CF Model Validation MAE: 0.6617\n",
      "CBF Model Validation RMSE: 2.5573\n",
      "CBF Model Validation MAE: 2.3362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pedro\\Desktop\\Github\\DS_340-Movies\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# For CF Model\n",
    "validationset = list(zip(cf_validation['userId'], cf_validation['movieId'], cf_validation['rating']))\n",
    "validation_predictions = algo.test(validationset)\n",
    "\n",
    "# Compute RMSE and MAE on validation set\n",
    "rmse_val = accuracy.rmse(validation_predictions, verbose=False)\n",
    "mae_val = accuracy.mae(validation_predictions, verbose=False)\n",
    "\n",
    "print(f\"CF Model Validation RMSE: {rmse_val:.4f}\")\n",
    "print(f\"CF Model Validation MAE: {mae_val:.4f}\")\n",
    "\n",
    "# For CBF Model\n",
    "# Merge cf_validation with cbf_validation\n",
    "validation_data = pd.merge(cf_validation, cbf_validation[['movieId', 'related']], on='movieId', how='inner')\n",
    "validation_tfidf = tfidf.transform(validation_data['related'])\n",
    "\n",
    "# Predict ratings for validation data\n",
    "predicted_ratings_val = []\n",
    "\n",
    "for idx, row in validation_data.iterrows():\n",
    "    user_id = row['userId']\n",
    "    movie_tfidf = validation_tfidf[idx]  # Sparse matrix\n",
    "    user_profile = user_profiles.get(user_id)  # Sparse matrix\n",
    "    \n",
    "    if user_profile is not None:\n",
    "        # Convert both user_profile and movie_tfidf to dense arrays\n",
    "        user_profile_dense = user_profile.A.flatten()  # Convert to dense array\n",
    "        movie_tfidf_dense = movie_tfidf.toarray().flatten()  # Convert to dense array\n",
    "\n",
    "        # Compute the dot product and append the scalar result\n",
    "        pred_rating = np.dot(user_profile_dense, movie_tfidf_dense)\n",
    "        predicted_ratings_val.append(pred_rating)\n",
    "    else:\n",
    "        predicted_ratings_val.append(np.nan)\n",
    "\n",
    "# Add the predicted ratings to the DataFrame\n",
    "validation_data['predicted_rating'] = predicted_ratings_val\n",
    "\n",
    "# Scale predicted ratings\n",
    "valid_predictions_val = validation_data.dropna(subset=['predicted_rating'])\n",
    "scaled_ratings_val = scaler.transform(valid_predictions_val['predicted_rating'].values.reshape(-1, 1))\n",
    "valid_predictions_val['predicted_rating'] = scaled_ratings_val.flatten()\n",
    "\n",
    "# Compute RMSE and MAE\n",
    "rmse_val = mean_squared_error(valid_predictions_val['rating'], valid_predictions_val['predicted_rating'], squared=False)\n",
    "mae_val = mean_absolute_error(valid_predictions_val['rating'], valid_predictions_val['predicted_rating'])\n",
    "\n",
    "print(f\"CBF Model Validation RMSE: {rmse_val:.4f}\")\n",
    "print(f\"CBF Model Validation MAE: {mae_val:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
