{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "## Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error as root_mean_squared_error \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, precision_score, recall_score, f1_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "movies = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\Github\\DS340-Midterm\\Small MovieLens\\movies.csv')\n",
    "ratings = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\Github\\DS340-Midterm\\Small MovieLens\\ratings.csv')\n",
    "tags = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\Github\\DS340-Midterm\\Small MovieLens\\tags.csv')\n",
    "\n",
    "# Keep necessary columns\n",
    "movies = movies[['movieId', 'title', 'genres']]\n",
    "ratings = ratings[['userId', 'movieId', 'rating']]\n",
    "tags = tags[['movieId', 'tag']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Movie Data\n",
    "\n",
    "Extract year, clean titles, and combine genres and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract year from title\n",
    "def extract_year(title):\n",
    "    match = re.search(r'\\((\\d{4})\\)', title)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to create 'year' column\n",
    "movies['year'] = movies['title'].apply(extract_year)\n",
    "\n",
    "# Clean the 'title' by removing the year and converting to lowercase\n",
    "movies['title_clean'] = movies['title'].apply(lambda x: re.sub(r'\\s*\\(\\d{4}\\)', '', x).lower())\n",
    "\n",
    "# Group tags by 'movieId' and concatenate them into a single string\n",
    "tags_grouped = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Merge tags with movies\n",
    "movies = pd.merge(movies, tags_grouped, on='movieId', how='left')\n",
    "movies['tag'] = movies['tag'].fillna('')\n",
    "\n",
    "# Combine genres, title_clean, tags, and year into the 'related' column\n",
    "movies['year_str'] = movies['year'].astype(str)\n",
    "movies['related'] = movies['genres'].str.replace('|', ' ') + ' ' + movies['title_clean'] + ' ' + movies['tag'] + ' ' + movies['year_str']\n",
    "\n",
    "# Preprocess the 'related' column\n",
    "movies['related'] = movies['related'].str.lower()\n",
    "movies['related'] = movies['related'].str.replace(r'\\d+', '', regex=True)\n",
    "movies['related'] = movies['related'].str.replace(r'[^a-z\\s]', '', regex=True)\n",
    "movies['related'] = movies['related'].str.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       1        1     4.0\n",
       "1       1        3     4.0\n",
       "2       1        6     4.0\n",
       "3       1       47     5.0\n",
       "4       1       50     5.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Content Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining, validation = train_test_split(ratings, test_size=0.1, random_state=42, stratify=ratings['userId'])\n",
    "training, testing = train_test_split(remaining, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(movies['related'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_index, val_index in kf.split(training):\n",
    "    cv_train_data = training.iloc[train_index]\n",
    "    cv_val_data = training.iloc[val_index]\n",
    "    # Train and evaluate your model here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_movies(movie_id, top_n=10):\n",
    "    idx = movies.index[movies['movieId'] == movie_id][0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_n+1]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return movies['movieId'].iloc[movie_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recommendations = {}\n",
    "\n",
    "for user_id in testing['userId'].unique():\n",
    "    user_movies = training[training['userId'] == user_id]['movieId']\n",
    "    rec_movies_list = []  # Use a list to collect similar movies\n",
    "\n",
    "    for movie_id in user_movies:\n",
    "        similar_movies = get_similar_movies(movie_id)\n",
    "        rec_movies_list.extend(similar_movies)  # Append similar movies to the list\n",
    "\n",
    "    # Convert the list to a pandas Series\n",
    "    rec_movies = pd.Series(rec_movies_list)\n",
    "\n",
    "    # Remove movies the user has already rated\n",
    "    rec_movies = rec_movies[~rec_movies.isin(user_movies)]\n",
    "\n",
    "    # Count occurrences of recommendations and get the top 10\n",
    "    user_recommendations[user_id] = rec_movies.value_counts().index[:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a rating threshold to consider a movie as \"liked\"\n",
    "rating_threshold = 4.0\n",
    "\n",
    "# Create binary relevance for test data\n",
    "testing['relevant'] = testing['rating'] >= rating_threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of estimating ratings\n",
    "def estimate_rating(user_id, movie_id):\n",
    "    similar_movies = get_similar_movies(movie_id)\n",
    "    similar_ratings = training[(training['userId'] == user_id) & (training['movieId'].isin(similar_movies))]['rating']\n",
    "    if not similar_ratings.empty:\n",
    "        return similar_ratings.mean()\n",
    "    else:\n",
    "        return training[training['userId'] == user_id]['rating'].mean()\n",
    "\n",
    "testing['predicted_rating'] = testing.apply(lambda x: estimate_rating(x['userId'], x['movieId']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0027050237743078 0.7508850234582828\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rmse = root_mean_squared_error(testing['rating'], testing['predicted_rating'])\n",
    "mae = mean_absolute_error(testing['rating'], testing['predicted_rating'])\n",
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary predictions based on whether the movie was recommended\n",
    "testing['predicted_relevant'] = testing.apply(\n",
    "    lambda x: x['movieId'] in user_recommendations.get(x['userId'], []), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6867469879518072 0.006502395619438741 0.01288281161713188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "precision = precision_score(testing['relevant'], testing['predicted_relevant'])\n",
    "recall = recall_score(testing['relevant'], testing['predicted_relevant'])\n",
    "f1 = f1_score(testing['relevant'], testing['predicted_relevant'])\n",
    "\n",
    "print(precision, recall, f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.0743\n",
      "Average Recall: 0.0141\n",
      "Average F1 Score: 0.0212\n",
      "Average RMSE: 0.9865\n",
      "Average MAE: 0.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\AppData\\Local\\Temp\\ipykernel_17032\\1717839758.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(calculate_user_metrics)\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate metrics for each user\n",
    "def calculate_user_metrics(group):\n",
    "    try:\n",
    "        precision = precision_score(group['relevant'], group['predicted_relevant'], zero_division=0)\n",
    "        recall = recall_score(group['relevant'], group['predicted_relevant'], zero_division=0)\n",
    "        f1 = f1_score(group['relevant'], group['predicted_relevant'], zero_division=0)\n",
    "    except ValueError:  # Handle cases where there are no positive samples\n",
    "        precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "    \n",
    "    rmse = root_mean_squared_error(group['rating'], group['predicted_rating'])\n",
    "    mae = mean_absolute_error(group['rating'], group['predicted_rating'])\n",
    "    \n",
    "    return pd.Series({'precision': precision, 'recall': recall, 'f1': f1, 'rmse': rmse, 'mae': mae})\n",
    "\n",
    "# Group by userId and compute metrics, excluding the grouping column explicitly\n",
    "user_metrics = (\n",
    "    testing.groupby('userId', group_keys=False)  # Ensures only grouped rows are passed\n",
    "    .apply(calculate_user_metrics)\n",
    ")\n",
    "\n",
    "# Compute average metrics\n",
    "avg_precision = user_metrics['precision'].mean()\n",
    "avg_recall = user_metrics['recall'].mean()\n",
    "avg_f1 = user_metrics['f1'].mean()\n",
    "avg_rmse = user_metrics['rmse'].mean()\n",
    "avg_mae = user_metrics['mae'].mean()\n",
    "\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
    "print(f\"Average MAE: {avg_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6867469879518072 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "# After each fold, store the metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Append metrics in each fold\n",
    "precision_scores.append(precision)\n",
    "# Similarly for other metrics...\n",
    "\n",
    "# After cross-validation\n",
    "print(f'Precision: {np.mean(precision_scores)} ± {np.std(precision_scores)}')\n",
    "# Similarly for other metrics...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6867469879518072 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "# After each fold, store the metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Append metrics in each fold\n",
    "precision_scores.append(precision)\n",
    "# Similarly for other metrics...\n",
    "\n",
    "# After cross-validation\n",
    "print(f'Precision: {np.mean(precision_scores)} ± {np.std(precision_scores)}')\n",
    "# Similarly for other metrics...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6867469879518072 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "# After each fold, store the metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Append metrics in each fold\n",
    "precision_scores.append(precision)\n",
    "# Similarly for other metrics...\n",
    "\n",
    "# After cross-validation\n",
    "print(f'Precision: {np.mean(precision_scores)} ± {np.std(precision_scores)}')\n",
    "# Similarly for other metrics...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-based model metrics\n",
    "content_results = pd.DataFrame({\n",
    "    'Model': ['Content-Based'],\n",
    "    'RMSE': [rmse],\n",
    "    'MAE': [mae],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Collaborative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import GridSearchCV, cross_validate, KFold\n",
    "from surprise.accuracy import rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(rating_scale=(ratings['rating'].min(), ratings['rating'].max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Surprise datasets\n",
    "data = Dataset.load_from_df(training[['userId', 'movieId', 'rating']], reader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150],\n",
    "    'n_epochs': [20, 30],\n",
    "    'lr_all': [0.005, 0.010],\n",
    "    'reg_all': [0.02, 0.05]\n",
    "}\n",
    "\n",
    "# Define the cross-validation iterator\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Perform grid search\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=kf, n_jobs=-1)\n",
    "\n",
    "gs.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE score: 0.8746784691583634\n",
      "Best hyperparameters:\n",
      "{'n_factors': 150, 'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Best RMSE score\n",
    "print(f\"Best RMSE score: {gs.best_score['rmse']}\")\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "print(gs.best_params['rmse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x18b048cb3d0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the training set\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Use the best model\n",
    "best_svd = gs.best_estimator['rmse']\n",
    "\n",
    "# Train the model on the full training set\n",
    "best_svd.fit(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the testset from the test data\n",
    "testing_data = Dataset.load_from_df(testing[['userId', 'movieId', 'rating']], reader)\n",
    "testingset = testing_data.build_full_trainset().build_testset()\n",
    "\n",
    "# Predict ratings\n",
    "predictions = best_svd.test(testingset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true and predicted ratings\n",
    "y_true = np.array([pred.r_ui for pred in predictions])\n",
    "y_pred = np.array([pred.est for pred in predictions])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE and MAE\n",
    "rmse = np.sqrt(root_mean_squared_error(y_true, y_pred))\n",
    "mae = mean_absolute_error(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rating threshold\n",
    "rating_threshold = 4.0\n",
    "\n",
    "# Convert ratings to binary relevance\n",
    "y_true_binary = (y_true >= rating_threshold).astype(int)\n",
    "y_pred_binary = (y_pred >= rating_threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Metrics:\n",
      "       RMSE       MAE  Precision    Recall  F1-Score\n",
      "0  0.859537  0.660689   0.820999  0.350559  0.491326\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to display the results\n",
    "results = pd.DataFrame({\n",
    "    'RMSE': [rmse],\n",
    "    'MAE': [mae],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1]\n",
    "})\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Metrics:\n",
      "   Fold      RMSE       MAE  Precision    Recall  F1-Score\n",
      "0     1  0.885114  0.682038   0.812895  0.346263  0.485655\n",
      "1     2  0.874633  0.673629   0.810702  0.347527  0.486503\n",
      "2     3  0.872095  0.670034   0.812078  0.347732  0.486951\n",
      "3     4  0.872325  0.670060   0.803193  0.330666  0.468468\n",
      "4     5  0.876103  0.671348   0.816533  0.333191  0.473264\n",
      "\n",
      "Average Cross-Validation Metrics:\n",
      "Fold         3.000000\n",
      "RMSE         0.876054\n",
      "MAE          0.673422\n",
      "Precision    0.811080\n",
      "Recall       0.341076\n",
      "F1-Score     0.480168\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store cross-validation results\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for trainset_cv, testset_cv in kf.split(data):\n",
    "    # Train the algorithm on trainset\n",
    "    best_svd.fit(trainset_cv)\n",
    "    \n",
    "    # Test the algorithm on testset\n",
    "    predictions_cv = best_svd.test(testset_cv)\n",
    "    \n",
    "    # Extract true and predicted ratings\n",
    "    y_true_cv = np.array([pred.r_ui for pred in predictions_cv])\n",
    "    y_pred_cv = np.array([pred.est for pred in predictions_cv])\n",
    "    \n",
    "    # Compute RMSE and MAE\n",
    "    rmse_cv = np.sqrt(root_mean_squared_error(y_true_cv, y_pred_cv))\n",
    "    mae_cv = mean_absolute_error(y_true_cv, y_pred_cv)\n",
    "    \n",
    "    # Convert to binary relevance\n",
    "    y_true_cv_binary = (y_true_cv >= rating_threshold).astype(int)\n",
    "    y_pred_cv_binary = (y_pred_cv >= rating_threshold).astype(int)\n",
    "    \n",
    "    # Compute Precision, Recall, and F1-Score\n",
    "    precision_cv = precision_score(y_true_cv_binary, y_pred_cv_binary, zero_division=0)\n",
    "    recall_cv = recall_score(y_true_cv_binary, y_pred_cv_binary, zero_division=0)\n",
    "    f1_cv = f1_score(y_true_cv_binary, y_pred_cv_binary, zero_division=0)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    rmse_list.append(rmse_cv)\n",
    "    mae_list.append(mae_cv)\n",
    "    precision_list.append(precision_cv)\n",
    "    recall_list.append(recall_cv)\n",
    "    f1_list.append(f1_cv)\n",
    "\n",
    "# Create a DataFrame with cross-validation metrics\n",
    "cv_results = pd.DataFrame({\n",
    "    'Fold': range(1, 6),\n",
    "    'RMSE': rmse_list,\n",
    "    'MAE': mae_list,\n",
    "    'Precision': precision_list,\n",
    "    'Recall': recall_list,\n",
    "    'F1-Score': f1_list\n",
    "})\n",
    "\n",
    "print(\"\\nCross-Validation Metrics:\")\n",
    "print(cv_results)\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_metrics = cv_results.mean(numeric_only=True)\n",
    "\n",
    "print(\"\\nAverage Cross-Validation Metrics:\")\n",
    "print(avg_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "      RMSE       MAE  Precision    Recall  F1-Score\n",
      "0  0.86899  0.668116   0.808086  0.326983  0.465576\n"
     ]
    }
   ],
   "source": [
    "# Build the validationset from the validation data\n",
    "validation_data = Dataset.load_from_df(validation[['userId', 'movieId', 'rating']], reader)\n",
    "validationset = validation_data.build_full_trainset().build_testset()\n",
    "\n",
    "\n",
    "# Predict ratings\n",
    "predictions_test = best_svd.test(validationset)\n",
    "\n",
    "# Extract true and predicted ratings\n",
    "y_true_test = np.array([pred.r_ui for pred in predictions_test])\n",
    "y_pred_test = np.array([pred.est for pred in predictions_test])\n",
    "\n",
    "# Compute RMSE and MAE\n",
    "rmse_test = np.sqrt(root_mean_squared_error(y_true_test, y_pred_test))\n",
    "mae_test = mean_absolute_error(y_true_test, y_pred_test)\n",
    "\n",
    "# Convert to binary relevance\n",
    "y_true_test_binary = (y_true_test >= rating_threshold).astype(int)\n",
    "y_pred_test_binary = (y_pred_test >= rating_threshold).astype(int)\n",
    "\n",
    "# Compute Precision, Recall, and F1-Score\n",
    "precision_test = precision_score(y_true_test_binary, y_pred_test_binary, zero_division=0)\n",
    "recall_test = recall_score(y_true_test_binary, y_pred_test_binary, zero_division=0)\n",
    "f1_test = f1_score(y_true_test_binary, y_pred_test_binary, zero_division=0)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "test_results = pd.DataFrame({\n",
    "    'RMSE': [rmse_test],\n",
    "    'MAE': [mae_test],\n",
    "    'Precision': [precision_test],\n",
    "    'Recall': [recall_test],\n",
    "    'F1-Score': [f1_test]\n",
    "})\n",
    "\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative filtering model metrics\n",
    "collaborative_results = pd.DataFrame({\n",
    "    'Model': ['Collaborative Filtering'],\n",
    "    'RMSE': [rmse],\n",
    "    'MAE': [mae],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for each model\n",
    "weight_content = 0.2\n",
    "weight_collaborative = 1- weight_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that 'testing' DataFrame contains both content-based and collaborative predictions\n",
    "\n",
    "# From content-based filtering\n",
    "testing_content = testing.copy()\n",
    "testing_content = testing_content[['userId', 'movieId', 'predicted_rating', 'relevant', 'predicted_relevant', 'rating']]\n",
    "\n",
    "# From collaborative filtering\n",
    "# We have 'predictions' list from collaborative filtering (Surprise library)\n",
    "# Convert the predictions to a DataFrame\n",
    "collab_preds = pd.DataFrame([(pred.uid, pred.iid, pred.est) for pred in predictions],\n",
    "                            columns=['userId', 'movieId', 'predicted_rating_collab'])\n",
    "\n",
    "# Merge the content-based and collaborative predictions on userId and movieId\n",
    "hybrid_data = pd.merge(testing_content, collab_preds, on=['userId', 'movieId'], how='inner')\n",
    "\n",
    "# Now, compute the hybrid predicted rating\n",
    "hybrid_data['predicted_rating_hybrid'] = (weight_content * hybrid_data['predicted_rating'] +\n",
    "                                          weight_collaborative * hybrid_data['predicted_rating_collab'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>relevant</th>\n",
       "      <th>predicted_rating</th>\n",
       "      <th>predicted_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82248</th>\n",
       "      <td>522</td>\n",
       "      <td>1265</td>\n",
       "      <td>4.5</td>\n",
       "      <td>True</td>\n",
       "      <td>3.734266</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>28</td>\n",
       "      <td>47629</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.016990</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84347</th>\n",
       "      <td>541</td>\n",
       "      <td>224</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>3.360656</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95803</th>\n",
       "      <td>600</td>\n",
       "      <td>30749</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.018987</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17706</th>\n",
       "      <td>111</td>\n",
       "      <td>94478</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating  relevant  predicted_rating  predicted_relevant\n",
       "82248     522     1265     4.5      True          3.734266               False\n",
       "4631       28    47629     2.0     False          3.016990               False\n",
       "84347     541      224     4.0      True          3.360656               False\n",
       "95803     600    30749     2.0     False          3.018987               False\n",
       "17706     111    94478     4.0      True          4.000000               False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.001351598477931 0.7508850234582828\n"
     ]
    }
   ],
   "source": [
    "# True ratings\n",
    "y_true_hybrid = hybrid_data['rating']\n",
    "\n",
    "# Hybrid predicted ratings\n",
    "y_pred_hybrid = hybrid_data['predicted_rating']\n",
    "\n",
    "# Compute RMSE and MAE\n",
    "rmse_hybrid = np.sqrt(root_mean_squared_error(y_true_hybrid, y_pred_hybrid))\n",
    "mae_hybrid = mean_absolute_error(y_true_hybrid, y_pred_hybrid)\n",
    "\n",
    "print(rmse_hybrid, mae_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8363376251788269 0.33344741044946385 0.47679634613816163\n"
     ]
    }
   ],
   "source": [
    "# Define rating threshold\n",
    "rating_threshold = 4.0\n",
    "\n",
    "# Convert true ratings to binary relevance\n",
    "hybrid_data['relevant'] = hybrid_data['rating'] >= rating_threshold\n",
    "\n",
    "# Convert hybrid predicted ratings to binary relevance\n",
    "hybrid_data['predicted_relevant_hybrid'] = hybrid_data['predicted_rating_hybrid'] >= rating_threshold\n",
    "\n",
    "# Compute Precision, Recall, and F1-Score\n",
    "precision_hybrid = precision_score(hybrid_data['relevant'], hybrid_data['predicted_relevant_hybrid'], zero_division=0)\n",
    "recall_hybrid = recall_score(hybrid_data['relevant'], hybrid_data['predicted_relevant_hybrid'], zero_division=0)\n",
    "f1_hybrid = f1_score(hybrid_data['relevant'], hybrid_data['predicted_relevant_hybrid'], zero_division=0)\n",
    "\n",
    "\n",
    "print(precision_hybrid, recall_hybrid, f1_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid Model Metrics:\n",
      "       RMSE       MAE  Precision    Recall  F1-Score\n",
      "0  1.001352  0.750885   0.836338  0.333447  0.476796\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to display the results\n",
    "hybrid_results = pd.DataFrame({\n",
    "    'RMSE': [rmse_hybrid],\n",
    "    'MAE': [mae_hybrid],\n",
    "    'Precision': [precision_hybrid],\n",
    "    'Recall': [recall_hybrid],\n",
    "    'F1-Score': [f1_hybrid]\n",
    "})\n",
    "\n",
    "print(\"\\nHybrid Model Metrics:\")\n",
    "print(hybrid_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Models:\n",
      "                     Model      RMSE       MAE  Precision    Recall  F1-Score\n",
      "0            Content-Based  1.002705  0.750885   0.686747  0.006502  0.012883\n",
      "1  Collaborative Filtering  0.859537  0.660689   0.820999  0.350559  0.491326\n",
      "2                   Hybrid  1.001352  0.750885   0.836338  0.333447  0.476796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Hybrid model metrics\n",
    "hybrid_results['Model'] = 'Hybrid'\n",
    "\n",
    "# Combine all results\n",
    "all_results = pd.concat([content_results, collaborative_results, hybrid_results], ignore_index=True)\n",
    "\n",
    "# Rearrange columns\n",
    "all_results = all_results[['Model', 'RMSE', 'MAE', 'Precision', 'Recall', 'F1-Score']]\n",
    "\n",
    "print(\"\\nComparison of Models:\")\n",
    "print(all_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
