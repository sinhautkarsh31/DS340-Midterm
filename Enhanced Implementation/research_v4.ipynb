{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Preparation\n",
    "\n",
    "This section prepares the raw `movies`, `ratings`, and `tags` datasets:\n",
    "1. Extracts necessary columns (e.g., `movieId`, `title`, `genres`).\n",
    "2. Cleans titles by removing years and converts them to lowercase.\n",
    "3. Groups and merges tags with movies.\n",
    "4. Combines genres, tags, and cleaned titles into a single feature (`related`) for vectorization.\n",
    "\n",
    "## Load and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Sklearn libraries for content-based filtering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "\n",
    "# Surprise library for collaborative filtering\n",
    "from surprise import Dataset, Reader\n",
    "from surprise import SVD\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise.model_selection import KFold as SurpriseKFold\n",
    "from surprise.accuracy import rmse, mae\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "movies = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\Github\\DS340-Midterm\\Small MovieLens\\movies.csv')\n",
    "ratings = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\Github\\DS340-Midterm\\Small MovieLens\\ratings.csv')\n",
    "tags = pd.read_csv(r'C:\\Users\\pedro\\Desktop\\Github\\DS340-Midterm\\Small MovieLens\\tags.csv')\n",
    "\n",
    "# Keep necessary columns\n",
    "movies = movies[['movieId', 'title', 'genres']]\n",
    "ratings = ratings[['userId', 'movieId', 'rating']]\n",
    "tags = tags[['movieId', 'tag']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Movie Data\n",
    "\n",
    "Extract year, clean titles, and combine genres and tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract year from title\n",
    "def extract_year(title):\n",
    "    match = re.search(r'\\((\\d{4})\\)', title)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Apply the function to create 'year' column\n",
    "movies['year'] = movies['title'].apply(extract_year)\n",
    "\n",
    "# Clean the 'title' by removing the year and converting to lowercase\n",
    "movies['title_clean'] = movies['title'].apply(lambda x: re.sub(r'\\s*\\(\\d{4}\\)', '', x).lower())\n",
    "\n",
    "# Group tags by 'movieId' and concatenate them into a single string\n",
    "tags_grouped = tags.groupby('movieId')['tag'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "\n",
    "# Merge tags with movies\n",
    "movies = pd.merge(movies, tags_grouped, on='movieId', how='left')\n",
    "movies['tag'] = movies['tag'].fillna('')\n",
    "\n",
    "# Combine genres, title_clean, tags, and year into the 'related' column\n",
    "movies['year_str'] = movies['year'].astype(str)\n",
    "movies['related'] = movies['genres'].str.replace('|', ' ') + ' ' + movies['title_clean'] + ' ' + movies['tag'] + ' ' + movies['year_str']\n",
    "\n",
    "# Preprocess the 'related' column\n",
    "movies['related'] = movies['related'].str.lower()\n",
    "movies['related'] = movies['related'].str.replace(r'\\d+', '', regex=True)\n",
    "movies['related'] = movies['related'].str.replace(r'[^a-z\\s]', '', regex=True)\n",
    "movies['related'] = movies['related'].str.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId  rating\n",
       "0       1        1     4.0\n",
       "1       1        3     4.0\n",
       "2       1        6     4.0\n",
       "3       1       47     5.0\n",
       "4       1       50     5.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies.head()\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Content Based\n",
    "\n",
    "1. Splits `ratings` data into training, validation, and testing sets.\n",
    "   - **Validation Set:** 10% of data.\n",
    "   - **Testing Set:** 20% of the remaining data.\n",
    "2. Uses `TfidfVectorizer` to create a matrix of movie-related text features.\n",
    "3. Computes pairwise cosine similarity to identify movies similar to a given movie.\n",
    "4. Implements a helper function (`get_similar_movies`) to fetch the top N most similar movies for a given `movieId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining, validation = train_test_split(ratings, test_size=0.1, random_state=42, stratify=ratings['userId'])\n",
    "training, testing = train_test_split(remaining, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(movies['related'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_movies(movie_id, top_n=10):\n",
    "    idx = movies.index[movies['movieId'] == movie_id][0]\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    sim_scores = sim_scores[1:top_n+1]\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "    return movies['movieId'].iloc[movie_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Recommendations\n",
    "1. For each user in the testing set:\n",
    "   - Finds movies the user has rated in the training set.\n",
    "   - Recommends the top 10 most frequent movies similar to those rated by the user.\n",
    "2. Ensures recommended movies are not already rated by the user.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recommendations = {}\n",
    "\n",
    "for user_id in testing['userId'].unique():\n",
    "    user_movies = training[training['userId'] == user_id]['movieId']\n",
    "    rec_movies_list = []  # Use a list to collect similar movies\n",
    "\n",
    "    for movie_id in user_movies:\n",
    "        similar_movies = get_similar_movies(movie_id)\n",
    "        rec_movies_list.extend(similar_movies)  # Append similar movies to the list\n",
    "\n",
    "    # Convert the list to a pandas Series\n",
    "    rec_movies = pd.Series(rec_movies_list)\n",
    "\n",
    "    # Remove movies the user has already rated\n",
    "    rec_movies = rec_movies[~rec_movies.isin(user_movies)]\n",
    "\n",
    "    # Count occurrences of recommendations and get the top 10\n",
    "    user_recommendations[user_id] = rec_movies.value_counts().index[:10]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a rating threshold to consider a movie as \"liked\"\n",
    "rating_threshold = 4.0\n",
    "\n",
    "# Create binary relevance for test data\n",
    "testing['relevant'] = testing['rating'] >= rating_threshold\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "1. Predicts ratings for the testing set using average ratings of similar movies.\n",
    "2. Computes RMSE and MAE to evaluate the accuracy of predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of estimating ratings\n",
    "def estimate_rating(user_id, movie_id):\n",
    "    similar_movies = get_similar_movies(movie_id)\n",
    "    similar_ratings = training[(training['userId'] == user_id) & (training['movieId'].isin(similar_movies))]['rating']\n",
    "    if not similar_ratings.empty:\n",
    "        return similar_ratings.mean()\n",
    "    else:\n",
    "        return training[training['userId'] == user_id]['rating'].mean()\n",
    "\n",
    "testing['predicted_rating'] = testing.apply(lambda x: estimate_rating(x['userId'], x['movieId']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.001351598477931 0.7508850234582828\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "rmse = root_mean_squared_error(testing['rating'], testing['predicted_rating'])\n",
    "mae = mean_absolute_error(testing['rating'], testing['predicted_rating'])\n",
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary predictions based on whether the movie was recommended\n",
    "testing['predicted_relevant'] = testing.apply(\n",
    "    lambda x: x['movieId'] in user_recommendations.get(x['userId'], []), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6867469879518072 0.006502395619438741 0.01288281161713188\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "precision = precision_score(testing['relevant'], testing['predicted_relevant'])\n",
    "recall = recall_score(testing['relevant'], testing['predicted_relevant'])\n",
    "f1 = f1_score(testing['relevant'], testing['predicted_relevant'])\n",
    "\n",
    "print(precision, recall, f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision: 0.0743\n",
      "Average Recall: 0.0141\n",
      "Average F1 Score: 0.0212\n",
      "Average RMSE: 0.9231\n",
      "Average MAE: 0.7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pedro\\AppData\\Local\\Temp\\ipykernel_45884\\4267982674.py:18: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(calculate_user_metrics)\n"
     ]
    }
   ],
   "source": [
    "# Define a function to calculate metrics for each user\n",
    "def calculate_user_metrics(group):\n",
    "    try:\n",
    "        precision = precision_score(group['relevant'], group['predicted_relevant'], zero_division=0)\n",
    "        recall = recall_score(group['relevant'], group['predicted_relevant'], zero_division=0)\n",
    "        f1 = f1_score(group['relevant'], group['predicted_relevant'], zero_division=0)\n",
    "    except ValueError:  # Handle cases where there are no positive samples\n",
    "        precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "    \n",
    "    rmse = root_mean_squared_error(group['rating'], group['predicted_rating'])\n",
    "    mae = mean_absolute_error(group['rating'], group['predicted_rating'])\n",
    "    \n",
    "    return pd.Series({'precision': precision, 'recall': recall, 'f1': f1, 'rmse': rmse, 'mae': mae})\n",
    "\n",
    "# Group by userId and compute metrics, excluding the grouping column explicitly\n",
    "user_metrics = (\n",
    "    testing.groupby('userId')  # Ensures only grouped rows are passed\n",
    "    .apply(calculate_user_metrics)\n",
    ")\n",
    "\n",
    "# Compute average metrics\n",
    "avg_precision = user_metrics['precision'].mean()\n",
    "avg_recall = user_metrics['recall'].mean()\n",
    "avg_f1 = user_metrics['f1'].mean()\n",
    "avg_rmse = user_metrics['rmse'].mean()\n",
    "avg_mae = user_metrics['mae'].mean()\n",
    "\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
    "print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "\n",
    "\n",
    "# Collaborative filtering model metrics\n",
    "content_results = pd.DataFrame({\n",
    "    'Model': ['Collaborative Filtering'],\n",
    "    'RMSE': [avg_rmse],\n",
    "    'MAE': [avg_mae],\n",
    "    'Precision': [avg_precision],\n",
    "    'Recall': [avg_recall],\n",
    "    'F1-Score': [avg_f1]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6867469879518072 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "# After each fold, store the metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Append metrics in each fold\n",
    "precision_scores.append(precision)\n",
    "# Similarly for other metrics...\n",
    "\n",
    "# After cross-validation\n",
    "print(f'Precision: {np.mean(precision_scores)} ± {np.std(precision_scores)}')\n",
    "# Similarly for other metrics...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6867469879518072 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "# After each fold, store the metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Append metrics in each fold\n",
    "precision_scores.append(precision)\n",
    "# Similarly for other metrics...\n",
    "\n",
    "# After cross-validation\n",
    "print(f'Precision: {np.mean(precision_scores)} ± {np.std(precision_scores)}')\n",
    "# Similarly for other metrics...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.6867469879518072 ± 0.0\n"
     ]
    }
   ],
   "source": [
    "# After each fold, store the metrics\n",
    "precision_scores = []\n",
    "recall_scores = []\n",
    "f1_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "# Append metrics in each fold\n",
    "precision_scores.append(precision)\n",
    "# Similarly for other metrics...\n",
    "\n",
    "# After cross-validation\n",
    "print(f'Precision: {np.mean(precision_scores)} ± {np.std(precision_scores)}')\n",
    "# Similarly for other metrics...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-Fold Cross-Validation\n",
    "1. Implements 5-fold cross-validation to train and evaluate the content-based filtering model.\n",
    "2. Splits the training data into 5 subsets (folds).\n",
    "3. In each fold:\n",
    "   - 4 folds are used for training (`cv_train_data`).\n",
    "   - 1 fold is used for validation (`cv_val_data`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of splits for cross-validation\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize lists to store cross-validation metrics\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "# Prepare the 'movies' DataFrame to be used in each fold\n",
    "movies_copy = movies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Fold 1/5...\n",
      "Fold 1 Metrics:\n",
      "RMSE: 1.0078, MAE: 0.7652, Precision: 0.5088, Recall: 0.0041, F1-Score: 0.0081\n",
      "\n",
      "Processing Fold 2/5...\n",
      "Fold 2 Metrics:\n",
      "RMSE: 1.0033, MAE: 0.7605, Precision: 0.6203, Recall: 0.0070, F1-Score: 0.0139\n",
      "\n",
      "Processing Fold 3/5...\n",
      "Fold 3 Metrics:\n",
      "RMSE: 0.9980, MAE: 0.7516, Precision: 0.5806, Recall: 0.0052, F1-Score: 0.0103\n",
      "\n",
      "Processing Fold 4/5...\n",
      "Fold 4 Metrics:\n",
      "RMSE: 1.0060, MAE: 0.7605, Precision: 0.6857, Recall: 0.0069, F1-Score: 0.0136\n",
      "\n",
      "Processing Fold 5/5...\n",
      "Fold 5 Metrics:\n",
      "RMSE: 1.0016, MAE: 0.7548, Precision: 0.7308, Recall: 0.0081, F1-Score: 0.0160\n"
     ]
    }
   ],
   "source": [
    "# Start the cross-validation loop\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(training)):\n",
    "    print(f\"\\nProcessing Fold {fold + 1}/{n_splits}...\")\n",
    "    \n",
    "    # Split the data into training and validation folds\n",
    "    cv_train_data = training.iloc[train_index]\n",
    "    cv_val_data = training.iloc[val_index]\n",
    "    \n",
    "    # Get the list of movies in cv_train_data and reset the index\n",
    "    cv_train_movies = movies[movies['movieId'].isin(cv_train_data['movieId'].unique())].reset_index(drop=True)\n",
    "    \n",
    "    # Fit the TF-IDF vectorizer on cv_train_movies\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(cv_train_movies['related'])\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # Create a mapping from movieId to index in cv_train_movies\n",
    "    movie_id_to_idx = pd.Series(cv_train_movies.index, index=cv_train_movies['movieId']).to_dict()\n",
    "    \n",
    "    # Define a function to get similar movies within the current fold\n",
    "    def get_similar_movies_cv(movie_id, top_n=10):\n",
    "        if movie_id not in movie_id_to_idx:\n",
    "            return []\n",
    "        idx = movie_id_to_idx[movie_id]\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        sim_scores = sim_scores[1:top_n+1]  # Exclude the movie itself\n",
    "        movie_indices = [i[0] for i in sim_scores]\n",
    "        return cv_train_movies['movieId'].iloc[movie_indices].tolist()\n",
    "    \n",
    "    # Prepare cv_val_data for predictions\n",
    "    cv_val_data = cv_val_data.copy()\n",
    "    cv_val_data['predicted_rating'] = np.nan\n",
    "    cv_val_data['predicted_relevant'] = False\n",
    "    \n",
    "    # Define rating threshold\n",
    "    rating_threshold = 4.0\n",
    "    cv_val_data['relevant'] = cv_val_data['rating'] >= rating_threshold\n",
    "    \n",
    "    # Generate recommendations and estimate ratings for cv_val_data\n",
    "    user_recommendations_cv = {}\n",
    "    \n",
    "    for user_id in cv_val_data['userId'].unique():\n",
    "        user_movies = cv_train_data[cv_train_data['userId'] == user_id]['movieId']\n",
    "        rec_movies_list = []\n",
    "        \n",
    "        for movie_id in user_movies:\n",
    "            similar_movies = get_similar_movies_cv(movie_id)\n",
    "            rec_movies_list.extend(similar_movies)\n",
    "        \n",
    "        rec_movies = pd.Series(rec_movies_list)\n",
    "        # Remove movies the user has already rated\n",
    "        rec_movies = rec_movies[~rec_movies.isin(user_movies)]\n",
    "        # Get top 10 recommendations\n",
    "        user_recommendations_cv[user_id] = rec_movies.value_counts().index[:10]\n",
    "    \n",
    "    # Estimate ratings and predicted relevance for cv_val_data\n",
    "    for idx, row in cv_val_data.iterrows():\n",
    "        user_id = row['userId']\n",
    "        movie_id = row['movieId']\n",
    "        \n",
    "        # Estimate rating\n",
    "        similar_movies = get_similar_movies_cv(movie_id)\n",
    "        similar_ratings = cv_train_data[(cv_train_data['userId'] == user_id) &\n",
    "                                        (cv_train_data['movieId'].isin(similar_movies))]['rating']\n",
    "        if not similar_ratings.empty:\n",
    "            predicted_rating = similar_ratings.mean()\n",
    "        else:\n",
    "            user_ratings = cv_train_data[cv_train_data['userId'] == user_id]['rating']\n",
    "            if not user_ratings.empty:\n",
    "                predicted_rating = user_ratings.mean()\n",
    "            else:\n",
    "                predicted_rating = cv_train_data['rating'].mean()\n",
    "        cv_val_data.at[idx, 'predicted_rating'] = predicted_rating\n",
    "        \n",
    "        # Predicted relevance\n",
    "        cv_val_data.at[idx, 'predicted_relevant'] = movie_id in user_recommendations_cv.get(user_id, [])\n",
    "    \n",
    "    # Compute metrics for the current fold\n",
    "    rmse_fold = np.sqrt(root_mean_squared_error(cv_val_data['rating'], cv_val_data['predicted_rating']))\n",
    "    mae_fold = mean_absolute_error(cv_val_data['rating'], cv_val_data['predicted_rating'])\n",
    "    precision_fold = precision_score(cv_val_data['relevant'], cv_val_data['predicted_relevant'], zero_division=0)\n",
    "    recall_fold = recall_score(cv_val_data['relevant'], cv_val_data['predicted_relevant'], zero_division=0)\n",
    "    f1_fold = f1_score(cv_val_data['relevant'], cv_val_data['predicted_relevant'], zero_division=0)\n",
    "    \n",
    "    # Append metrics to the lists\n",
    "    rmse_list.append(rmse_fold)\n",
    "    mae_list.append(mae_fold)\n",
    "    precision_list.append(precision_fold)\n",
    "    recall_list.append(recall_fold)\n",
    "    f1_list.append(f1_fold)\n",
    "    \n",
    "    # Print metrics for the current fold\n",
    "    print(f\"Fold {fold + 1} Metrics:\")\n",
    "    print(f\"RMSE: {rmse_fold:.4f}, MAE: {mae_fold:.4f}, Precision: {precision_fold:.4f}, Recall: {recall_fold:.4f}, F1-Score: {f1_fold:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Metrics:\n",
      "   Fold      RMSE       MAE  Precision    Recall  F1-Score\n",
      "0     1  0.866018  0.661682   0.824173  0.365184  0.506114\n",
      "1     2  0.851497  0.652010   0.815249  0.351686  0.491392\n",
      "2     3  0.847085  0.649629   0.828666  0.361017  0.502928\n",
      "3     4  0.853872  0.653935   0.826149  0.358663  0.500179\n",
      "4     5  0.865285  0.662636   0.822017  0.349053  0.490026\n",
      "\n",
      "Average Cross-Validation Metrics:\n",
      "   Fold      RMSE       MAE  Precision    Recall  F1-Score          Model\n",
      "0   3.0  0.856751  0.655978   0.823251  0.357121  0.498128  Content-Based\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with cross-validation metrics\n",
    "cv_results = pd.DataFrame({\n",
    "    'Fold': range(1, n_splits +1),\n",
    "    'RMSE': rmse_list,\n",
    "    'MAE': mae_list,\n",
    "    'Precision': precision_list,\n",
    "    'Recall': recall_list,\n",
    "    'F1-Score': f1_list\n",
    "})\n",
    "\n",
    "print(\"\\nCross-Validation Metrics:\")\n",
    "print(cv_results)\n",
    "\n",
    "# Compute average metrics\n",
    "# For Content-Based Model\n",
    "avg_metrics = cv_results.mean(numeric_only=True).to_frame().T\n",
    "avg_metrics['Model'] = 'Content-Based'\n",
    "\n",
    "print(\"\\nAverage Cross-Validation Metrics:\")\n",
    "print(avg_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Collaborative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import KFold\n",
    "\n",
    "\n",
    "# Define reader and load data\n",
    "reader = Reader(rating_scale=(ratings['rating'].min(), ratings['rating'].max()))\n",
    "data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering with Grid Search\n",
    "1. Performs grid search to find optimal hyperparameters for the SVD algorithm:\n",
    "   - Number of latent factors (`n_factors`).\n",
    "   - Number of epochs (`n_epochs`).\n",
    "   - Learning rate (`lr_all`).\n",
    "   - Regularization (`reg_all`).\n",
    "2. Uses 5-fold cross-validation to evaluate models.\n",
    "3. Selects the model with the best RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_factors': [50, 100, 150],\n",
    "    'n_epochs': [20, 30],\n",
    "    'lr_all': [0.005, 0.01],\n",
    "    'reg_all': [0.02, 0.05]\n",
    "}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model to the dataset\n",
    "gs.fit(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE score: 0.8572431459131458\n",
      "Best hyperparameters:\n",
      "{'n_factors': 150, 'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Best RMSE score\n",
    "print(f\"Best RMSE score: {gs.best_score['rmse']}\")\n",
    "\n",
    "# Best hyperparameters\n",
    "print(\"Best hyperparameters:\")\n",
    "print(gs.best_params['rmse'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<surprise.prediction_algorithms.matrix_factorization.SVD at 0x226000bfbd0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the training set\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "# Use the best model\n",
    "best_svd = gs.best_estimator['rmse']\n",
    "\n",
    "# Train the model on the full training set\n",
    "best_svd.fit(trainset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the testset from the test data\n",
    "testing_data = Dataset.load_from_df(testing[['userId', 'movieId', 'rating']], reader)\n",
    "testingset = testing_data.build_full_trainset().build_testset()\n",
    "\n",
    "# Predict ratings\n",
    "predictions = best_svd.test(testingset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true and predicted ratings\n",
    "y_true = np.array([pred.r_ui for pred in predictions])\n",
    "y_pred = np.array([pred.est for pred in predictions])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate RMSE and MAE\n",
    "rmse = root_mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rating threshold\n",
    "rating_threshold = 4.0\n",
    "\n",
    "# Convert ratings to binary relevance\n",
    "y_true_binary = (y_true >= rating_threshold).astype(int)\n",
    "y_pred_binary = (y_pred >= rating_threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Metrics:\n",
      "       RMSE       MAE  Precision    Recall  F1-Score\n",
      "0  0.447033  0.345019   0.967654  0.569929  0.717352\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to display the results\n",
    "results = pd.DataFrame({\n",
    "    'RMSE': [rmse],\n",
    "    'MAE': [mae],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1]\n",
    "})\n",
    "\n",
    "print(\"\\nTesting Metrics:\")\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Validation\n",
    "1. Uses the best SVD model to predict ratings for the test set.\n",
    "2. Computes RMSE, MAE, Precision, Recall, and F1-Score to evaluate performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Metrics:\n",
      "   Fold      RMSE       MAE  Precision    Recall  F1-Score\n",
      "0     1  0.880222  0.678225   0.825354  0.346546  0.488136\n",
      "1     2  0.873649  0.671061   0.818644  0.346237  0.486650\n",
      "2     3  0.871624  0.670733   0.802984  0.342098  0.479789\n",
      "3     4  0.873311  0.671021   0.807221  0.338668  0.477149\n",
      "4     5  0.876259  0.670357   0.811761  0.341873  0.481122\n",
      "\n",
      "Average Cross-Validation Metrics:\n",
      "   Fold      RMSE       MAE  Precision    Recall  F1-Score          Model\n",
      "0   3.0  0.875013  0.672279   0.813193  0.343084  0.482569  Collaborative\n"
     ]
    }
   ],
   "source": [
    "from surprise.model_selection import KFold\n",
    "from surprise.accuracy import rmse, mae\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Define lists to store cross-validation metrics\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "precision_list = []\n",
    "recall_list = []\n",
    "f1_list = []\n",
    "\n",
    "# Define rating threshold\n",
    "rating_threshold = 4.0\n",
    "\n",
    "# Initialize Surprise KFold\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "for trainset, testset in kf.split(data):\n",
    "    # Train the algorithm on the training set\n",
    "    best_svd.fit(trainset)\n",
    "    \n",
    "    # Test the algorithm on the testing set\n",
    "    predictions = best_svd.test(testset)\n",
    "    \n",
    "    # Extract true and predicted ratings\n",
    "    y_true = np.array([pred.r_ui for pred in predictions])\n",
    "    y_pred = np.array([pred.est for pred in predictions])\n",
    "    \n",
    "    # Compute RMSE and MAE\n",
    "    rmse_cv = rmse(predictions, verbose=False)\n",
    "    mae_cv = mae(predictions, verbose=False)\n",
    "    \n",
    "    # Convert to binary relevance\n",
    "    y_true_binary = (y_true >= rating_threshold).astype(int)\n",
    "    y_pred_binary = (y_pred >= rating_threshold).astype(int)\n",
    "    \n",
    "    # Compute Precision, Recall, and F1-Score\n",
    "    precision_cv = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "    recall_cv = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "    f1_cv = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    rmse_list.append(rmse_cv)\n",
    "    mae_list.append(mae_cv)\n",
    "    precision_list.append(precision_cv)\n",
    "    recall_list.append(recall_cv)\n",
    "    f1_list.append(f1_cv)\n",
    "\n",
    "# Create a DataFrame with cross-validation metrics\n",
    "cv_results = pd.DataFrame({\n",
    "    'Fold': range(1, 6),\n",
    "    'RMSE': rmse_list,\n",
    "    'MAE': mae_list,\n",
    "    'Precision': precision_list,\n",
    "    'Recall': recall_list,\n",
    "    'F1-Score': f1_list\n",
    "})\n",
    "\n",
    "# Print Cross-Validation Metrics\n",
    "print(\"\\nCross-Validation Metrics:\")\n",
    "print(cv_results)\n",
    "\n",
    "# For Collaborative Filtering Model\n",
    "avg_metrics_collaborative = cv_results.mean(numeric_only=True).to_frame().T\n",
    "avg_metrics_collaborative['Model'] = 'Collaborative'\n",
    "\n",
    "print(\"\\nAverage Cross-Validation Metrics:\")\n",
    "print(avg_metrics_collaborative)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Metrics:\n",
      "       RMSE      MAE  Precision    Recall  F1-Score\n",
      "0  0.523762  0.38002   0.956103  0.514185  0.668731\n"
     ]
    }
   ],
   "source": [
    "# Build the validationset from the validation data\n",
    "validation_data = Dataset.load_from_df(validation[['userId', 'movieId', 'rating']], reader)\n",
    "validationset = validation_data.build_full_trainset().build_testset()\n",
    "\n",
    "\n",
    "# Predict ratings\n",
    "predictions_test = best_svd.test(validationset)\n",
    "\n",
    "# Extract true and predicted ratings\n",
    "y_true_test = np.array([pred.r_ui for pred in predictions_test])\n",
    "y_pred_test = np.array([pred.est for pred in predictions_test])\n",
    "\n",
    "# Compute RMSE and MAE\n",
    "rmse_test = root_mean_squared_error(y_true_test, y_pred_test)\n",
    "mae_test = mean_absolute_error(y_true_test, y_pred_test)\n",
    "\n",
    "# Convert to binary relevance\n",
    "y_true_test_binary = (y_true_test >= rating_threshold).astype(int)\n",
    "y_pred_test_binary = (y_pred_test >= rating_threshold).astype(int)\n",
    "\n",
    "# Compute Precision, Recall, and F1-Score\n",
    "precision_test = precision_score(y_true_test_binary, y_pred_test_binary, zero_division=0)\n",
    "recall_test = recall_score(y_true_test_binary, y_pred_test_binary, zero_division=0)\n",
    "f1_test = f1_score(y_true_test_binary, y_pred_test_binary, zero_division=0)\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "test_results = pd.DataFrame({\n",
    "    'RMSE': [rmse_test],\n",
    "    'MAE': [mae_test],\n",
    "    'Precision': [precision_test],\n",
    "    'Recall': [recall_test],\n",
    "    'F1-Score': [f1_test]\n",
    "})\n",
    "\n",
    "print(\"\\nValidation Metrics:\")\n",
    "print(test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative filtering model metrics\n",
    "collaborative_results = pd.DataFrame({\n",
    "    'Model': ['Collaborative Filtering'],\n",
    "    'RMSE': [rmse],\n",
    "    'MAE': [mae],\n",
    "    'Precision': [precision],\n",
    "    'Recall': [recall],\n",
    "    'F1-Score': [f1]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weights for each model\n",
    "weight_content = 0.2\n",
    "weight_collaborative = 1- weight_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that 'testing' DataFrame contains both content-based and collaborative predictions\n",
    "\n",
    "# From content-based filtering\n",
    "testing_content = testing.copy()\n",
    "testing_content = testing_content[['userId', 'movieId', 'predicted_rating', 'relevant', 'predicted_relevant', 'rating']]\n",
    "\n",
    "# From collaborative filtering\n",
    "# We have 'predictions' list from collaborative filtering (Surprise library)\n",
    "# Convert the predictions to a DataFrame\n",
    "collab_preds = pd.DataFrame([(pred.uid, pred.iid, pred.est) for pred in predictions],\n",
    "                            columns=['userId', 'movieId', 'predicted_rating_collab'])\n",
    "\n",
    "# Merge the content-based and collaborative predictions on userId and movieId\n",
    "hybrid_data = pd.merge(testing_content, collab_preds, on=['userId', 'movieId'], how='inner')\n",
    "\n",
    "# Now, compute the hybrid predicted rating\n",
    "hybrid_data['predicted_rating_hybrid'] = (weight_content * hybrid_data['predicted_rating'] +\n",
    "                                          weight_collaborative * hybrid_data['predicted_rating_collab'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>relevant</th>\n",
       "      <th>predicted_rating</th>\n",
       "      <th>predicted_relevant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82248</th>\n",
       "      <td>522</td>\n",
       "      <td>1265</td>\n",
       "      <td>4.5</td>\n",
       "      <td>True</td>\n",
       "      <td>3.734266</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4631</th>\n",
       "      <td>28</td>\n",
       "      <td>47629</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.016990</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84347</th>\n",
       "      <td>541</td>\n",
       "      <td>224</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>3.360656</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95803</th>\n",
       "      <td>600</td>\n",
       "      <td>30749</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>3.018987</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17706</th>\n",
       "      <td>111</td>\n",
       "      <td>94478</td>\n",
       "      <td>4.0</td>\n",
       "      <td>True</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       userId  movieId  rating  relevant  predicted_rating  predicted_relevant\n",
       "82248     522     1265     4.5      True          3.734266               False\n",
       "4631       28    47629     2.0     False          3.016990               False\n",
       "84347     541      224     4.0      True          3.360656               False\n",
       "95803     600    30749     2.0     False          3.018987               False\n",
       "17706     111    94478     4.0      True          4.000000               False"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0229241405079337 0.76762793550662\n"
     ]
    }
   ],
   "source": [
    "# True ratings\n",
    "y_true_hybrid = hybrid_data['rating']\n",
    "\n",
    "# Hybrid predicted ratings\n",
    "y_pred_hybrid = hybrid_data['predicted_rating']\n",
    "\n",
    "# Compute RMSE and MAE\n",
    "rmse_hybrid = root_mean_squared_error(y_true_hybrid, y_pred_hybrid)\n",
    "mae_hybrid = mean_absolute_error(y_true_hybrid, y_pred_hybrid)\n",
    "\n",
    "print(rmse_hybrid, mae_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8506787330316742 0.31438127090301005 0.4590964590964591\n"
     ]
    }
   ],
   "source": [
    "# Define rating threshold\n",
    "rating_threshold = 4.0\n",
    "\n",
    "# Convert true ratings to binary relevance\n",
    "hybrid_data['relevant'] = hybrid_data['rating'] >= rating_threshold\n",
    "\n",
    "# Convert hybrid predicted ratings to binary relevance\n",
    "hybrid_data['predicted_relevant_hybrid'] = hybrid_data['predicted_rating_hybrid'] >= rating_threshold\n",
    "\n",
    "# Compute Precision, Recall, and F1-Score\n",
    "precision_hybrid = precision_score(hybrid_data['relevant'], hybrid_data['predicted_relevant_hybrid'], zero_division=0)\n",
    "recall_hybrid = recall_score(hybrid_data['relevant'], hybrid_data['predicted_relevant_hybrid'], zero_division=0)\n",
    "f1_hybrid = f1_score(hybrid_data['relevant'], hybrid_data['predicted_relevant_hybrid'], zero_division=0)\n",
    "\n",
    "\n",
    "print(precision_hybrid, recall_hybrid, f1_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid Model Metrics:\n",
      "       RMSE       MAE  Precision    Recall  F1-Score\n",
      "0  1.022924  0.767628   0.850679  0.314381  0.459096\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to display the results\n",
    "hybrid_results = pd.DataFrame({\n",
    "    'RMSE': [rmse_hybrid],\n",
    "    'MAE': [mae_hybrid],\n",
    "    'Precision': [precision_hybrid],\n",
    "    'Recall': [recall_hybrid],\n",
    "    'F1-Score': [f1_hybrid]\n",
    "})\n",
    "\n",
    "print(\"\\nHybrid Model Metrics:\")\n",
    "print(hybrid_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Models:\n",
      "                     Model                                   RMSE  \\\n",
      "0  Collaborative Filtering                                 0.9231   \n",
      "1  Collaborative Filtering  <function rmse at 0x00000225E6BE7380>   \n",
      "2                   Hybrid                               1.022924   \n",
      "\n",
      "                                    MAE  Precision    Recall  F1-Score  \n",
      "0                              0.739989   0.074317  0.014127  0.021159  \n",
      "1  <function mae at 0x00000225E6BE74C0>   0.967654  0.569929  0.717352  \n",
      "2                              0.767628   0.850679  0.314381  0.459096  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Hybrid model metrics\n",
    "hybrid_results['Model'] = 'Hybrid'\n",
    "\n",
    "# Combine all results\n",
    "all_results = pd.concat([content_results, collaborative_results, hybrid_results], ignore_index=True)\n",
    "\n",
    "# Rearrange columns\n",
    "all_results = all_results[['Model', 'RMSE', 'MAE', 'Precision', 'Recall', 'F1-Score']]\n",
    "\n",
    "print(\"\\nComparison of Models:\")\n",
    "print(all_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Fold 1/5 for Hybrid Model...\n",
      "Fold 1 Metrics:\n",
      "RMSE: 0.8802, MAE: 0.6789, Precision: 0.8374, Recall: 0.3106, F1-Score: 0.4531\n",
      "\n",
      "Processing Fold 2/5 for Hybrid Model...\n",
      "Fold 2 Metrics:\n",
      "RMSE: 0.8760, MAE: 0.6769, Precision: 0.8352, Recall: 0.3038, F1-Score: 0.4455\n",
      "\n",
      "Processing Fold 3/5 for Hybrid Model...\n",
      "Fold 3 Metrics:\n",
      "RMSE: 0.8720, MAE: 0.6696, Precision: 0.8424, Recall: 0.3074, F1-Score: 0.4505\n",
      "\n",
      "Processing Fold 4/5 for Hybrid Model...\n",
      "Fold 4 Metrics:\n",
      "RMSE: 0.8749, MAE: 0.6728, Precision: 0.8392, Recall: 0.2975, F1-Score: 0.4393\n",
      "\n",
      "Processing Fold 5/5 for Hybrid Model...\n",
      "Fold 5 Metrics:\n",
      "RMSE: 0.8782, MAE: 0.6700, Precision: 0.8321, Recall: 0.2935, F1-Score: 0.4339\n",
      "\n",
      "Average Metrics Across Folds:\n",
      "RMSE: 0.8763, MAE: 0.6736, Precision: 0.8373, Recall: 0.3026, F1-Score: 0.4445\n"
     ]
    }
   ],
   "source": [
    "from surprise.model_selection import KFold\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error, precision_score, recall_score, f1_score\n",
    "from surprise.accuracy import rmse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store cross-validation metrics\n",
    "rmse_list_hybrid = []\n",
    "mae_list_hybrid = []\n",
    "precision_list_hybrid = []\n",
    "recall_list_hybrid = []\n",
    "f1_list_hybrid = []\n",
    "\n",
    "# Define weights for the hybrid model (adjust as needed)\n",
    "weight_content = 0.5\n",
    "weight_collaborative = 1- weight_content\n",
    "\n",
    "# Reader object for Surprise\n",
    "reader = Reader(rating_scale=(ratings['rating'].min(), ratings['rating'].max()))\n",
    "\n",
    "# Use Surprise's KFold for cross-validation\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "\n",
    "# Convert training data to Surprise Dataset\n",
    "data = Dataset.load_from_df(training[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "for fold, (trainset, testset) in enumerate(kf.split(data)):\n",
    "    print(f\"\\nProcessing Fold {fold + 1}/5 for Hybrid Model...\")\n",
    "    \n",
    "    # ----------------------\n",
    "    # Collaborative Filtering Model\n",
    "    # ----------------------\n",
    "    \n",
    "    # Train the collaborative filtering model\n",
    "    algo_collab = SVD(n_factors=150, n_epochs=30, lr_all=0.01, reg_all=0.05)\n",
    "    algo_collab.fit(trainset)\n",
    "    \n",
    "    # Test collaborative filtering model\n",
    "    predictions_collab = algo_collab.test(testset)\n",
    "    \n",
    "    # Extract predictions and true ratings for collaborative filtering\n",
    "    collab_preds = pd.DataFrame([(pred.uid, pred.iid, pred.est, pred.r_ui) for pred in predictions_collab],\n",
    "                                columns=['userId', 'movieId', 'predicted_rating_collab', 'rating'])\n",
    "    collab_preds['userId'] = collab_preds['userId'].astype(int)\n",
    "    collab_preds['movieId'] = collab_preds['movieId'].astype(int)\n",
    "    \n",
    "    # ----------------------\n",
    "    # Content-Based Model\n",
    "    # ----------------------\n",
    "    \n",
    "    # Filter movies in the current fold\n",
    "    fold_movies = movies[movies['movieId'].isin(collab_preds['movieId'].unique())].reset_index(drop=True)\n",
    "    \n",
    "    # Fit the TF-IDF vectorizer\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(fold_movies['related'])\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # Map movieId to indices in the current fold\n",
    "    movie_id_to_idx = pd.Series(fold_movies.index, index=fold_movies['movieId']).to_dict()\n",
    "    \n",
    "    # Define function to estimate content-based ratings\n",
    "    def estimate_content_rating(user_id, movie_id):\n",
    "        if movie_id not in movie_id_to_idx:\n",
    "            return np.nan\n",
    "        idx = movie_id_to_idx[movie_id]\n",
    "        sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)[1:11]\n",
    "        similar_movies = [fold_movies.iloc[i[0]]['movieId'] for i in sim_scores]\n",
    "        \n",
    "        # Get user ratings for similar movies\n",
    "        user_ratings = collab_preds[(collab_preds['userId'] == user_id) &\n",
    "                                    (collab_preds['movieId'].isin(similar_movies))]['rating']\n",
    "        return user_ratings.mean() if not user_ratings.empty else collab_preds[collab_preds['userId'] == user_id]['rating'].mean()\n",
    "\n",
    "    # Estimate content-based ratings\n",
    "    collab_preds['predicted_rating_content'] = collab_preds.apply(\n",
    "        lambda row: estimate_content_rating(row['userId'], row['movieId']), axis=1)\n",
    "    \n",
    "    # ----------------------\n",
    "    # Combine Predictions for Hybrid Model\n",
    "    # ----------------------\n",
    "    \n",
    "    # Compute hybrid predictions\n",
    "    collab_preds['predicted_rating_hybrid'] = (\n",
    "        weight_content * collab_preds['predicted_rating_content'] +\n",
    "        weight_collaborative * collab_preds['predicted_rating_collab']\n",
    "    )\n",
    "    \n",
    "    # Define rating threshold for relevance\n",
    "    collab_preds['relevant'] = collab_preds['rating'] >= 4.0\n",
    "    collab_preds['predicted_relevant_hybrid'] = collab_preds['predicted_rating_hybrid'] >= 4.0\n",
    "    \n",
    "    # Compute metrics for the current fold\n",
    "    rmse_fold = rmse(predictions_collab, verbose=False)\n",
    "    mae_fold = mean_absolute_error(collab_preds['rating'], collab_preds['predicted_rating_hybrid'])\n",
    "    precision_fold = precision_score(collab_preds['relevant'], collab_preds['predicted_relevant_hybrid'], zero_division=0)\n",
    "    recall_fold = recall_score(collab_preds['relevant'], collab_preds['predicted_relevant_hybrid'], zero_division=0)\n",
    "    f1_fold = f1_score(collab_preds['relevant'], collab_preds['predicted_relevant_hybrid'], zero_division=0)\n",
    "    \n",
    "    # Append metrics to lists\n",
    "    rmse_list_hybrid.append(rmse_fold)\n",
    "    mae_list_hybrid.append(mae_fold)\n",
    "    precision_list_hybrid.append(precision_fold)\n",
    "    recall_list_hybrid.append(recall_fold)\n",
    "    f1_list_hybrid.append(f1_fold)\n",
    "    \n",
    "    # Print metrics for the current fold\n",
    "    print(f\"Fold {fold + 1} Metrics:\")\n",
    "    print(f\"RMSE: {rmse_fold:.4f}, MAE: {mae_fold:.4f}, Precision: {precision_fold:.4f}, \"\n",
    "          f\"Recall: {recall_fold:.4f}, F1-Score: {f1_fold:.4f}\")\n",
    "\n",
    "# Calculate average metrics\n",
    "print(\"\\nAverage Metrics Across Folds:\")\n",
    "print(f\"RMSE: {np.mean(rmse_list_hybrid):.4f}, MAE: {np.mean(mae_list_hybrid):.4f}, \"\n",
    "      f\"Precision: {np.mean(precision_list_hybrid):.4f}, Recall: {np.mean(recall_list_hybrid):.4f}, \"\n",
    "      f\"F1-Score: {np.mean(f1_list_hybrid):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Cross-Validation Metrics for Hybrid Model:\n",
      "RMSE: 0.8763 ± 0.0028\n",
      "MAE: 0.6736 ± 0.0037\n",
      "Precision: 0.8373 ± 0.0035\n",
      "Recall: 0.3026 ± 0.0063\n",
      "F1-Score: 0.4445 ± 0.0071\n"
     ]
    }
   ],
   "source": [
    "# Convert lists to numpy arrays for convenience\n",
    "rmse_array = np.array(rmse_list_hybrid)\n",
    "mae_array = np.array(mae_list_hybrid)\n",
    "precision_array = np.array(precision_list_hybrid)\n",
    "recall_array = np.array(recall_list_hybrid)\n",
    "f1_array = np.array(f1_list_hybrid)\n",
    "\n",
    "# Compute means\n",
    "mean_rmse = rmse_array.mean()\n",
    "mean_mae = mae_array.mean()\n",
    "mean_precision = precision_array.mean()\n",
    "mean_recall = recall_array.mean()\n",
    "mean_f1 = f1_array.mean()\n",
    "\n",
    "# Compute standard deviations\n",
    "std_rmse = rmse_array.std()\n",
    "std_mae = mae_array.std()\n",
    "std_precision = precision_array.std()\n",
    "std_recall = recall_array.std()\n",
    "std_f1 = f1_array.std()\n",
    "\n",
    "# Print the results\n",
    "print(\"\\nAverage Cross-Validation Metrics for Hybrid Model:\")\n",
    "print(f\"RMSE: {mean_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "print(f\"MAE: {mean_mae:.4f} ± {std_mae:.4f}\")\n",
    "print(f\"Precision: {mean_precision:.4f} ± {std_precision:.4f}\")\n",
    "print(f\"Recall: {mean_recall:.4f} ± {std_recall:.4f}\")\n",
    "print(f\"F1-Score: {mean_f1:.4f} ± {std_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid Model Cross-Validation Metrics:\n",
      "   Fold      RMSE       MAE  Precision    Recall  F1-Score\n",
      "0     1  0.880243  0.678912   0.837405  0.310589  0.453119\n",
      "1     2  0.875970  0.676916   0.835238  0.303799  0.445542\n",
      "2     3  0.872038  0.669575   0.842439  0.307426  0.450466\n",
      "3     4  0.874900  0.672817   0.839178  0.297514  0.439287\n",
      "4     5  0.878160  0.669984   0.832123  0.293481  0.433923\n",
      "\n",
      "Average Cross-Validation Metrics:\n",
      "   Fold      RMSE       MAE  Precision    Recall  F1-Score   Model\n",
      "0   3.0  0.876262  0.673641   0.837276  0.302562  0.444467  Hybrid\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with cross-validation metrics\n",
    "cv_results_hybrid = pd.DataFrame({\n",
    "    'Fold': range(1, n_splits +1),\n",
    "    'RMSE': rmse_list_hybrid,\n",
    "    'MAE': mae_list_hybrid,\n",
    "    'Precision': precision_list_hybrid,\n",
    "    'Recall': recall_list_hybrid,\n",
    "    'F1-Score': f1_list_hybrid\n",
    "})\n",
    "\n",
    "print(\"\\nHybrid Model Cross-Validation Metrics:\")\n",
    "print(cv_results_hybrid)\n",
    "\n",
    "\n",
    "# For Hybrid Model\n",
    "avg_metrics_hybrid = cv_results_hybrid.mean(numeric_only=True).to_frame().T\n",
    "avg_metrics_hybrid['Model'] = 'Hybrid'\n",
    "\n",
    "print(\"\\nAverage Cross-Validation Metrics:\")\n",
    "print(avg_metrics_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Fold      RMSE       MAE  Precision    Recall  F1-Score          Model\n",
      "0   3.0  0.856751  0.655978   0.823251  0.357121  0.498128  Content-Based \n",
      "    Fold      RMSE       MAE  Precision    Recall  F1-Score  \\\n",
      "0   3.0  0.874667  0.672173   0.814409  0.342893  0.482581   \n",
      "\n",
      "                     Model  \n",
      "0  Collaborative Filtering   \n",
      "    Fold      RMSE       MAE  Precision    Recall  F1-Score   Model\n",
      "0   3.0  0.876262  0.673641   0.837276  0.302562  0.444467  Hybrid \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(avg_metrics.head(),\"\\n\",avg_metrics_collaborative,\"\\n\", avg_metrics_hybrid, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Metrics Comparison:\n",
      "                     Model  Fold      RMSE       MAE  Precision    Recall  \\\n",
      "0            Content-Based   3.0  0.856751  0.655978   0.823251  0.357121   \n",
      "1  Collaborative Filtering   3.0  0.875013  0.672279   0.813193  0.343084   \n",
      "2                   Hybrid   3.0  0.876262  0.673641   0.837276  0.302562   \n",
      "\n",
      "   F1-Score  \n",
      "0  0.498128  \n",
      "1  0.482569  \n",
      "2  0.444467  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Combine the cross-validation results\n",
    "avg_metrics_all = pd.concat([avg_metrics, avg_metrics_collaborative, avg_metrics_hybrid], ignore_index=True)\n",
    "avg_metrics_all = avg_metrics_all[['Model', 'Fold', 'RMSE', 'MAE', 'Precision', 'Recall', 'F1-Score']]\n",
    "\n",
    "print(\"\\nCross-Validation Metrics Comparison:\")\n",
    "print(avg_metrics_all)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
